{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HbWKisU6So6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "main_path = \"data\"\n",
    "os.chdir(main_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVNmc81hxehU"
   },
   "source": [
    "## Keyword Extraction & POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTNsi2sJgsk7"
   },
   "outputs": [],
   "source": [
    "#!cd \"/content/drive/My Drive/\"\n",
    "#!wget https://nlp.stanford.edu/software/stanford-postagger-2018-10-16.zip\n",
    "#!unzip stanford-postagger-2018-10-16.zip\n",
    "#!mv stanford-postagger-2018-10-16.zip stanford-postagger\n",
    "#!python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "UxzguzN8bTnO",
    "outputId": "5c174b21-3424-4b85-b042-9c886f540648"
   },
   "outputs": [],
   "source": [
    "# Import Stanford POS Tagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "_path_to_model = 'stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger'\n",
    "_path_to_jar = 'stanford-postagger-2018-10-16/stanford-postagger.jar'\n",
    "st = StanfordPOSTagger(model_filename=_path_to_model, path_to_jar=_path_to_jar)\n",
    "\n",
    "# Install RAKE for keyword extraction\n",
    "!pip3 install python-rake\n",
    "import RAKE\n",
    "Rake = RAKE.Rake(\"stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "ADkSah4NJE_V",
    "outputId": "705c2426-ffd3-4cd2-e7a0-c08cf6dc9be4"
   },
   "outputs": [],
   "source": [
    "# Import Stanford POS Tagger\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "_path_to_model = 'stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger'\n",
    "_path_to_jar = 'stanford-postagger-2018-10-16/stanford-postagger.jar'\n",
    "stanford_tagger = StanfordPOSTagger(model_filename=_path_to_model, path_to_jar=_path_to_jar)\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from nltk.data import load\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0BcstMCy8XF"
   },
   "outputs": [],
   "source": [
    "def extract_keywords_and_POS(prompt):\n",
    "    POS_dict = {}\n",
    "    try:\n",
    "        tagged_prompt = st.tag(prompt.split())\n",
    "    except:\n",
    "        print(\"ERROR PROMPT: \", prompt)\n",
    "        return False\n",
    "    else:\n",
    "        for pair in tagged_prompt:\n",
    "            POS_dict[pair[0]] = pair[1]\n",
    "        keywords_dict = {}\n",
    "        #format: Rake.run(prompt, minCharacters = X, maxWords = Y, minFrequency = Z)\n",
    "        keywords = Rake.run(prompt)\n",
    "        for pair in keywords:\n",
    "            words = pair[0].split()\n",
    "            for word in words:\n",
    "                try:\n",
    "                    keywords_dict[word] = POS_dict[word]\n",
    "                except:\n",
    "                    pass\n",
    "        return keywords_dict\n",
    "\n",
    "#Example:\n",
    "prompt = \"first thing we do , let's fight all the lawyers\"\n",
    "keywords_dict = extract_keywords_and_POS(prompt)\n",
    "print(keywords_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iifenUIKNkv0"
   },
   "source": [
    "## WordNet: Hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "31Gb_WOrNm1x",
    "outputId": "72f1555f-a6cb-45a1-cc69-a46ed8510503"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_hyponyms(word, pos):\n",
    "    hypos_lst = []\n",
    "    try:\n",
    "        s = wordnet.synsets(word, pos)[0]\n",
    "    except:\n",
    "        try:\n",
    "            s = wordnet.synsets(word)[0]\n",
    "        except:\n",
    "            return hypos_lst\n",
    "    if s.name() == 'restrain.v.01':\n",
    "        print(\"RESTRAIN ENCOUNTERED (hypos)\")\n",
    "        return hypos_lst\n",
    "    hypos = lambda s:s.hyponyms()\n",
    "    hypos = list(s.closure(hypos))\n",
    "    for syn in hypos:\n",
    "        for l in syn.lemmas():\n",
    "            if l.name().lower() != word:\n",
    "                hypos_lst.append(l.name().lower())\n",
    "    return list(dict.fromkeys(hypos_lst))\n",
    "\n",
    "#Example:\n",
    "print(get_hyponyms(\"person\", \"n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HpVW70jlTTkU",
    "outputId": "a81f16ac-8958-4ed1-ea12-f0522201924d"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def single_prompt_helper(keywords_lst, keywords_dict, fnc, chosen_nums):\n",
    "    counter = 1\n",
    "    chosen_keywords_lst = []\n",
    "    chosen_replacements_lst = []\n",
    "    for i in range(0,len(keywords_lst)):\n",
    "        if counter <= max(chosen_nums):\n",
    "            keyword = keywords_lst[i]\n",
    "            keyword_pos = keywords_dict[keyword][0].lower()\n",
    "            if keyword_pos == 'j':\n",
    "                keyword_pos = 'a'\n",
    "            candidates = fnc(keyword, keyword_pos)\n",
    "            if len(candidates) != 0:\n",
    "                counter += 1\n",
    "                chosen_keywords_lst.append(keyword)\n",
    "                chosen_replacement = random.choice(candidates)\n",
    "                chosen_replacements_lst.append(chosen_replacement)\n",
    "        else:\n",
    "            return chosen_keywords_lst, chosen_replacements_lst\n",
    "    return chosen_keywords_lst, chosen_replacements_lst\n",
    "\n",
    "\n",
    "def single_prompt_wordnet(prompt, nums_lst):\n",
    "    original_prompt = prompt\n",
    "    hyponyms_prompt_lst = []\n",
    "    keywords_dict = extract_keywords_and_POS(prompt)\n",
    "    if keywords_dict == False:\n",
    "        return []\n",
    "    keywords_lst = list(keywords_dict.keys())\n",
    "    num_keywords = len(keywords_lst)\n",
    "    prompt_hyponym = original_prompt\n",
    "    chosen_keywords, chosen_hyponyms = single_prompt_helper(keywords_lst, keywords_dict, get_hyponyms, nums_lst)\n",
    "    counter = 1\n",
    "    for chosen_word, chosen_hypo in zip(chosen_keywords, chosen_hyponyms):\n",
    "        prompt_hyponym = re.sub(r\"\\b%s\\b\" % chosen_word, chosen_hypo, prompt_hyponym)\n",
    "        if counter in nums_lst:\n",
    "            hyponyms_prompt_lst.append(re.sub('_',' ',prompt_hyponym))\n",
    "        counter += 1\n",
    "    return hyponyms_prompt_lst\n",
    "\n",
    "\n",
    "#Example:\n",
    "random.seed(54321)\n",
    "nums_lst = [1,2,3]\n",
    "prompt = \"an immortal being is explaining\"\n",
    "hypos_lst = single_prompt_wordnet(prompt,nums_lst)\n",
    "print(hypos_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3RmpINVbV6x6"
   },
   "outputs": [],
   "source": [
    "def main_wordnet(input_file, output_file, nums_lst):\n",
    "    hypos_prompt_lst = []\n",
    "    hypos_counter = 0\n",
    "    with open(input_file) as in_f:\n",
    "        input_prompts = in_f.readlines()\n",
    "    counter = 0\n",
    "    for prompt in input_prompts:\n",
    "        hypos_lst = single_prompt_wordnet(prompt.strip('\\n'), nums_lst)\n",
    "        if hypos_lst is not None and len(hypos_lst) > 0:\n",
    "            hypos_counter += 1\n",
    "            hypos_prompt_lst.append('\\t'.join(hypos_lst)+'\\n')\n",
    "        else:\n",
    "            hypos_prompt_lst.append('<blank>\\n')\n",
    "        if counter % 100 == 0:\n",
    "            print(counter)\n",
    "            write_wordnet_prompts(hypos_prompt_lst, output_file)\n",
    "            hypos_prompt_lst = []\n",
    "        counter += 1\n",
    "    write_wordnet_prompts(hypos_prompt_lst, output_file)\n",
    "    print(\"Final hyponym lines: \", hypos_counter)\n",
    "    return hypos_prompt_lst\n",
    "\n",
    "\n",
    "def write_wordnet_prompts(hypos_lst, output_file):\n",
    "    f = open(output_file, 'a')\n",
    "    print(\"Writing output prompts to file...\")\n",
    "    f.writelines(hypos_lst)\n",
    "    f.close()\n",
    "    print(\"Output prompts written to file\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "F4xrUzfjChu8",
    "outputId": "9e746af1-2ed5-4b06-d4e5-376d3e112139"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "random.seed(54321)\n",
    "nums_lst = [1,2,3]\n",
    "input_file = 'yelp_train.txt'\n",
    "output_file = 'yelp_train_hyponyms.txt'\n",
    "\n",
    "start = time.time()\n",
    "hypos_lst = main_wordnet(input_file, output_file, nums_lst)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "PROSEAM_WordNet_Hyponyms (Random Seed).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
