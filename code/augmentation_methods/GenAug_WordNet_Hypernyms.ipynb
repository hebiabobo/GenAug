{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HbWKisU6So6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "main_path = \"data\"\n",
    "os.chdir(main_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVNmc81hxehU"
   },
   "source": [
    "## Keyword Extraction & POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTNsi2sJgsk7"
   },
   "outputs": [],
   "source": [
    "#!wget https://nlp.stanford.edu/software/stanford-postagger-2018-10-16.zip\n",
    "#!unzip stanford-postagger-2018-10-16.zip\n",
    "#!mv stanford-postagger-2018-10-16.zip stanford-postagger\n",
    "#!python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "UxzguzN8bTnO",
    "outputId": "9bfb7933-6afd-44d5-ecdc-6896e42aa507"
   },
   "outputs": [],
   "source": [
    "# Import Stanford POS Tagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "_path_to_model = 'stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger'\n",
    "_path_to_jar = 'stanford-postagger-2018-10-16/stanford-postagger.jar'\n",
    "st = StanfordPOSTagger(model_filename=_path_to_model, path_to_jar=_path_to_jar)\n",
    "\n",
    "# Install RAKE for keyword extraction\n",
    "!pip3 install python-rake\n",
    "import RAKE\n",
    "Rake = RAKE.Rake(\"stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "ADkSah4NJE_V",
    "outputId": "71ee77b5-03c3-4980-e3c3-47647eb31b0b"
   },
   "outputs": [],
   "source": [
    "# Import Stanford POS Tagger\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "_path_to_model = 'stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger'\n",
    "_path_to_jar = 'stanford-postagger-2018-10-16/stanford-postagger.jar'\n",
    "stanford_tagger = StanfordPOSTagger(model_filename=_path_to_model, path_to_jar=_path_to_jar)\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from nltk.data import load\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0BcstMCy8XF"
   },
   "outputs": [],
   "source": [
    "def extract_keywords_and_POS(prompt):\n",
    "    POS_dict = {}\n",
    "    try:\n",
    "        tagged_prompt = st.tag(prompt.split())\n",
    "    except:\n",
    "        print(\"ERROR PROMPT: \", prompt)\n",
    "        return False\n",
    "    else:\n",
    "        for pair in tagged_prompt:\n",
    "            POS_dict[pair[0]] = pair[1]\n",
    "        keywords_dict = {}\n",
    "        #format: Rake.run(prompt, minCharacters = X, maxWords = Y, minFrequency = Z)\n",
    "        keywords = Rake.run(prompt)\n",
    "        for pair in keywords:\n",
    "            words = pair[0].split()\n",
    "            for word in words:\n",
    "                try:\n",
    "                    keywords_dict[word] = POS_dict[word]\n",
    "                except:\n",
    "                    pass\n",
    "        return keywords_dict\n",
    "\n",
    "#Example:\n",
    "prompt = \"first thing we do , let's fight all the lawyers\"\n",
    "keywords_dict = extract_keywords_and_POS(prompt)\n",
    "print(keywords_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iifenUIKNkv0"
   },
   "source": [
    "## WordNet: Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "31Gb_WOrNm1x",
    "outputId": "70c13d64-d1ae-43ae-cc75-ee27f3db10d6"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_hypernyms(word, pos):\n",
    "    hypers_lst = []\n",
    "    try:\n",
    "        s = wordnet.synsets(word, pos)[0]\n",
    "    except:\n",
    "        try:\n",
    "            s = wordnet.synsets(word)[0]\n",
    "        except:\n",
    "            return hypers_lst\n",
    "    if s.name() == 'restrain.v.01':\n",
    "        print(\"RESTRAIN ENCOUNTERED (hypers)\")\n",
    "        return hypers_lst\n",
    "    hypers = lambda s:s.hypernyms()\n",
    "    hypers = list(s.closure(hypers))\n",
    "    for syn in hypers:\n",
    "        for l in syn.lemmas():\n",
    "            if l.name().lower() != word:\n",
    "                hypers_lst.append(l.name().lower())\n",
    "    return list(dict.fromkeys(hypers_lst))\n",
    "\n",
    "#Example:\n",
    "print(get_hypernyms(\"person\", \"n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HpVW70jlTTkU",
    "outputId": "dcc2e67e-b96b-43f6-8ed7-bb88260771ac"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def single_prompt_helper(keywords_lst, keywords_dict, fnc, chosen_nums):\n",
    "    counter = 1\n",
    "    chosen_keywords_lst = []\n",
    "    chosen_replacements_lst = []\n",
    "    for i in range(0,len(keywords_lst)):\n",
    "        if counter <= max(chosen_nums):\n",
    "            keyword = keywords_lst[i]\n",
    "            keyword_pos = keywords_dict[keyword][0].lower()\n",
    "            if keyword_pos == 'j':\n",
    "                keyword_pos = 'a'\n",
    "            candidates = fnc(keyword, keyword_pos)\n",
    "            if len(candidates) != 0:\n",
    "                counter += 1\n",
    "                chosen_keywords_lst.append(keyword)\n",
    "                chosen_replacement = candidates[0]\n",
    "                chosen_replacements_lst.append(chosen_replacement)\n",
    "        else:\n",
    "            return chosen_keywords_lst, chosen_replacements_lst\n",
    "    return chosen_keywords_lst, chosen_replacements_lst\n",
    "\n",
    "\n",
    "def single_prompt_wordnet(prompt, nums_lst):\n",
    "    original_prompt = prompt\n",
    "    hypernyms_prompt_lst = []\n",
    "    keywords_dict = extract_keywords_and_POS(prompt)\n",
    "    if keywords_dict == False:\n",
    "        return []\n",
    "    keywords_lst = list(keywords_dict.keys())\n",
    "    num_keywords = len(keywords_lst)\n",
    "    prompt_hypernym = original_prompt\n",
    "    chosen_keywords, chosen_hypernyms = single_prompt_helper(keywords_lst, keywords_dict, get_hypernyms, nums_lst)    \n",
    "    counter = 1\n",
    "    for chosen_word, chosen_hyper in zip(chosen_keywords, chosen_hypernyms):\n",
    "        prompt_hypernym = re.sub(r\"\\b%s\\b\" % chosen_word, chosen_hyper, prompt_hypernym)\n",
    "        if counter in nums_lst:\n",
    "            hypernyms_prompt_lst.append(re.sub('_',' ',prompt_hypernym))\n",
    "        counter += 1\n",
    "    return hypernyms_prompt_lst\n",
    "\n",
    "\n",
    "#Example:\n",
    "nums_lst = [1,2,3]\n",
    "prompt = \"an immortal being is explaining\"\n",
    "hypers_lst = single_prompt_wordnet(prompt,nums_lst)\n",
    "print(hypers_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3RmpINVbV6x6"
   },
   "outputs": [],
   "source": [
    "def main_wordnet(input_file, output_file, nums_lst):\n",
    "    hypers_prompt_lst = []\n",
    "    hypers_counter = 0\n",
    "    with open(input_file) as in_f:\n",
    "        input_prompts = in_f.readlines()\n",
    "    counter = 0\n",
    "    for prompt in input_prompts:\n",
    "        hypers_lst = single_prompt_wordnet(prompt.strip('\\n'), nums_lst)\n",
    "        if hypers_lst is not None and len(hypers_lst) > 0:\n",
    "            hypers_counter += 1\n",
    "            hypers_prompt_lst.append('\\t'.join(hypers_lst)+'\\n')\n",
    "        else:\n",
    "            hypers_prompt_lst.append('<blank>\\n')\n",
    "        if counter % 100 == 0:\n",
    "            print(counter)\n",
    "            write_wordnet_prompts(hypers_prompt_lst, output_file)\n",
    "            hypers_prompt_lst = []\n",
    "        counter += 1\n",
    "    write_wordnet_prompts(hypers_prompt_lst, output_file)\n",
    "    print(\"Final hypernym lines: \", hypers_counter)\n",
    "    return hypers_prompt_lst\n",
    "\n",
    "\n",
    "def write_wordnet_prompts(hypers_lst, output_file):\n",
    "    f = open(output_file, 'a')\n",
    "    print(\"Writing output prompts to file...\")\n",
    "    f.writelines(hypers_lst)\n",
    "    f.close()\n",
    "    print(\"Output prompts written to file\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "F4xrUzfjChu8",
    "outputId": "0cb948c2-5662-4363-d1b5-036fd5740022"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "random.seed(54321)\n",
    "nums_lst = [1,2,3]\n",
    "input_file = 'yelp_train.txt'\n",
    "output_file = 'yelp_train_hypernyms.txt'\n",
    "\n",
    "start = time.time()\n",
    "hypers_lst = main_wordnet(input_file, output_file, nums_lst)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "PROSEAM_WordNet_Hypernyms (Random Seed).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
