{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlybB3UqHXYZ"
   },
   "source": [
    "# FOR TRAINING:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUDAZVqKGt7r"
   },
   "source": [
    "## Setup SMERTI Train/Val Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGj2ahf0Swm2"
   },
   "source": [
    "### Total 30-word windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpJCJv1nGlog"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "main_path = \"data\"\n",
    "os.chdir(main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZMGR3h1vGwFy"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import io\n",
    "\n",
    "\n",
    "def get_smerti_data(file, amount):\n",
    "    f = open(file, 'r')\n",
    "    lines = f.readlines()\n",
    "    chosen_lines = random.sample(lines, amount)\n",
    "    total_lines = []\n",
    "    for l in chosen_lines:\n",
    "        words = l.strip('\\n').split()\n",
    "        if len(words) <= 25:\n",
    "            total_lines.append(l)\n",
    "            continue\n",
    "        else:\n",
    "            num_chunks = math.ceil((len(words)-10)/20)+1\n",
    "            if len(words) % 20 <= 5: # and len([x for x in words[-(len(words)) % 20] if x not in punctuation]) \n",
    "                num_chunks = num_chunks-1\n",
    "        for i in range(0, num_chunks):\n",
    "            if i == 0:\n",
    "                chunk = words[0:20]\n",
    "                total_lines.append(' '.join(chunk)+'\\n')\n",
    "            else:\n",
    "                if len(words[10+(i-1)*20+30:]) <= 5:\n",
    "                    chunk = words[10+(i-1)*20:]\n",
    "                else:\n",
    "                    chunk = words[10+(i-1)*20:10+(i-1)*20+30]\n",
    "                chunk_text = ' '.join(chunk[10:])\n",
    "                if len(chunk) > 10 and len(chunk_text.strip()) > 0:\n",
    "                    total_lines.append(' '.join(chunk)+'\\n')\n",
    "    random.shuffle(total_lines)\n",
    "    return total_lines\n",
    "\n",
    "\n",
    "def write_lines(lines, path):\n",
    "    f = io.open(path, \"w\", encoding = 'utf-8')\n",
    "    print(\"Currently writing lines to file ...\")\n",
    "    f.writelines(lines)\n",
    "    f.close()\n",
    "    print(\"Lines successfully written to file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ChU-ZWGzgReA",
    "outputId": "7711921f-d83e-42b8-bbd6-1e7026275e41"
   },
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "\n",
    "input_file = 'yelp_val.txt'\n",
    "output_file = 'yelp_val_SMERTI.txt'\n",
    "lines = get_smerti_data(input_file, 7500)\n",
    "write_lines(lines, output_file)\n",
    "\n",
    "input_file = 'yelp_train.txt'\n",
    "output_file = 'yelp_train_SMERTI.txt'\n",
    "lines = get_smerti_data(input_file, 25000)\n",
    "write_lines(lines, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VATaCXYQHJ_p"
   },
   "source": [
    "## SMERTI Masking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rwo97C3DHN5O"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import io, json, os, collections, pprint, time\n",
    "import re\n",
    "from string import punctuation\n",
    "import random\n",
    "from random import sample\n",
    "import math\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "\n",
    "def mask_all(path):\n",
    "    mask_list = []\n",
    "    f = io.open(path, encoding = 'utf-8')\n",
    "    lines = f.readlines()\n",
    "    total = len(lines)\n",
    "    counter = 0\n",
    "    print(\"Currently reading lines from file ...\")\n",
    "    for l in lines:\n",
    "        if 0 <= counter <= int(round(total / 3)):\n",
    "            masked_text = mask_text(l, 0.20)\n",
    "        elif int(round(total / 3)) < counter <= int(round((total * 2) / 3)):\n",
    "            masked_text = mask_text(l, 0.40)\n",
    "        else:\n",
    "            masked_text = mask_text(l, 0.60)\n",
    "        mask_list.append(masked_text + '\\n')\n",
    "        counter += 1\n",
    "    return mask_list\n",
    "\n",
    "\n",
    "def mask_text(line, value):\n",
    "    word_list = (line.rstrip()).split()\n",
    "    num = int(round(len(word_list) * value))\n",
    "    mask_locs = set(sample(range(len(word_list)), num))\n",
    "    masked = list(('[mask]' if i in mask_locs and word_list[i] not in punctuation else c for i,c in enumerate(word_list)))\n",
    "    masked_groups = mask_groupings(masked)\n",
    "    masked_text = ' '.join(masked_groups)\n",
    "    return masked_text\n",
    "\n",
    "\n",
    "def mask_groupings(masked_list):\n",
    "    masked_group_list = []\n",
    "    previous_element = \"\"\n",
    "    for element in masked_list:\n",
    "        if element != \"[mask]\":\n",
    "            masked_group_list.append(element)\n",
    "        elif element == \"[mask]\":\n",
    "            if element != previous_element:\n",
    "                masked_group_list.append(element)\n",
    "        previous_element = element\n",
    "    return masked_group_list\n",
    "\n",
    "\n",
    "def write_file(lst, path):\n",
    "    f = io.open(path, \"w\", encoding = 'utf-8')\n",
    "    print(\"Currently writing lines to file ...\")\n",
    "    f.writelines(lst)\n",
    "    f.close()\n",
    "    print(\"Lines successfully written to file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "8RVBaO6j4-ew",
    "outputId": "5cd9b39a-157d-44ce-eabc-b27981fdc83c"
   },
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "\n",
    "input_path = 'yelp_train_SMERTI.txt'\n",
    "output_path = 'yelp_train_SMERTI_masked.txt'\n",
    "mask_lst = mask_all(input_path)\n",
    "write_file(mask_lst, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5Nf1Y66HZ7x"
   },
   "source": [
    "# FOR INFERENCE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7l23R0beojK-"
   },
   "source": [
    "## Split Training Data into Batches of 1000 and Combine SMERTI Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is necessary as SMERTI inference takes a while to run, so we split the data into batches of 1000 examples and then recombine the outputs afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62YJLUOHoowg"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def chunk_file(path):\n",
    "    f = open(path, 'r')\n",
    "    lines = f.readlines()\n",
    "    num_chunks = math.ceil(len(lines)/1000)\n",
    "    print(\"Writing chunks to files...\")\n",
    "    for i in range(num_chunks):\n",
    "        chunk = lines[i*1000:(i+1)*1000]\n",
    "        out_f = open('{}_p{}_SMERTI.txt'.format(path, i+1), 'w')\n",
    "        out_f.writelines(chunk)\n",
    "        out_f.close()\n",
    "    print(\"Chunks written to files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxKtsQV2p_gP"
   },
   "outputs": [],
   "source": [
    "input_path = 'yelp_train.txt'\n",
    "chunk_file(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuVNeiMn6g6A"
   },
   "outputs": [],
   "source": [
    "def combine_files(num_input_files, output_path):\n",
    "    final_lines = []\n",
    "    files = []\n",
    "    for i in range(num_input_files):\n",
    "        input_file = open('yelp_train_p{}_SMERTI_outputs.txt'.format(i+1),'r')\n",
    "        files.append(input_file)\n",
    "    for f in files:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            final_lines.append(l.strip('\\n')+'\\n')\n",
    "    output_file = open(output_path, 'w')\n",
    "    output_file.writelines(final_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "l6V6B8g_6hqz",
    "outputId": "12d11d67-b050-49e4-cdbd-6ef4ae456bb8"
   },
   "outputs": [],
   "source": [
    "num_input_files = 50\n",
    "output_path = 'yelp_train_SMERTI_outputs.txt'\n",
    "combine_files(num_input_files, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MrXvXgP1HoSI"
   },
   "source": [
    "## Get Training Vocab and Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtFf0FLaHxyY"
   },
   "outputs": [],
   "source": [
    "# Spacy for POS tagging\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BLvFeiHFH3yU",
    "outputId": "280469ba-b949-444f-8264-858344c86de0"
   },
   "outputs": [],
   "source": [
    "def extract_POS(word):\n",
    "    tokenized_word = nlp(word)\n",
    "    for token in tokenized_word:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "#Example:\n",
    "prompt = \"i hate this restaurant . the food is horrible !\"\n",
    "word = 'cooking'\n",
    "POS_dict = extract_POS(word)\n",
    "print(POS_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMNp0jvTH9_o"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "def get_vocab(file):\n",
    "    word_dict = defaultdict(int)\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        words = line.strip().split()\n",
    "        for w in [i for i in words if i not in punctuation]:\n",
    "            word_dict[w] += 1\n",
    "    print(len(word_dict))\n",
    "    word_dict = {k: v for k, v in sorted(word_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def get_nouns(vocab_file):\n",
    "    word_dict = defaultdict(int)\n",
    "    with open(vocab_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    counter = 0\n",
    "    for line in lines:\n",
    "        counter += 1\n",
    "        if counter % 1000 == 0:\n",
    "            print(counter)\n",
    "        elements = line.split('\\t')\n",
    "        word = elements[0]\n",
    "        count = int(elements[1].strip())\n",
    "        if extract_POS(word) == True:\n",
    "            word_dict[word] = count\n",
    "    print(len(word_dict))\n",
    "    word_dict = {k: v for k, v in sorted(word_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def write_words(word_dict, file):\n",
    "    f = open(file, 'w')\n",
    "    for word, count in word_dict.items():\n",
    "        f.write(word + '\\t' + str(count) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "x71tO3qSH_kj",
    "outputId": "77c4aacc-c6ac-4363-8e54-9c5e638a250c"
   },
   "outputs": [],
   "source": [
    "input_file = 'SMERTI/yelp_train_SMERTI.txt'\n",
    "output_file_vocab = 'SMERTI/yelp_train_SMERTI_vocab.txt'\n",
    "output_file_nouns = 'SMERTI/yelp_train_SMERTI_nouns.txt'\n",
    "\n",
    "word_dict = get_vocab(input_file)\n",
    "write_words(word_dict, output_file_vocab)\n",
    "\n",
    "noun_dict = get_nouns(output_file_vocab)\n",
    "write_words(noun_dict, output_file_nouns)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SMERTI_Functions (PROSEAM).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
